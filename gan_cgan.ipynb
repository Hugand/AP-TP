{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T00:23:24.006788Z","iopub.status.busy":"2023-06-12T00:23:24.005982Z","iopub.status.idle":"2023-06-12T00:23:27.204126Z","shell.execute_reply":"2023-06-12T00:23:27.203141Z","shell.execute_reply.started":"2023-06-12T00:23:24.006731Z"},"id":"rqFohEj4qqPC","trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","from torch import nn\n","import torchvision.transforms.functional as F\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","import pickle\n","from torch.optim.lr_scheduler import ExponentialLR\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yh_s7DYBrPj2","outputId":"efa96396-09db-44f8-abb2-d487dee83872"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T00:23:28.357083Z","iopub.status.busy":"2023-06-12T00:23:28.356369Z","iopub.status.idle":"2023-06-12T00:23:44.534892Z","shell.execute_reply":"2023-06-12T00:23:44.533988Z","shell.execute_reply.started":"2023-06-12T00:23:28.357028Z"},"id":"O3Ynfj4_qqPE","trusted":true},"outputs":[],"source":["class FacesDataset(Dataset):\n","    def __init__(self, imgs, labels):\n","        self.imgs = imgs\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.imgs)\n","\n","    def __getitem__(self, idx):\n","        return self.imgs[idx], self.labels[idx]\n","\n","def load_dataset(batch_size, augment=True):\n","    faces = torch.load('/kaggle/input/ap-cfggan-ds/faces_sex_images_2.pt').permute((0, 3, 1, 2))\n","    labels = torch.load('/kaggle/input/ap-cfggan-ds/labels_sex_images_2.pt')\n","    \n","    if augment:\n","        men = faces[:3367]\n","        indices = torch.randperm(len(men))[:5721-3367]\n","        selected_men = men[indices]\n","        selected_men = F.hflip(selected_men)\n","        faces = torch.cat((faces[:len(faces)-25], selected_men[:len(selected_men)-25]), 0)\n","        labels = torch.cat((labels[:len(labels)-25], torch.ones(len(selected_men))[:len(selected_men)-25]), 0)\n","    \n","    print(faces.shape, labels.shape)\n","    \n","    dataloader = DataLoader(FacesDataset(faces, labels), batch_size, True, pin_memory=True)\n","    dataloader_faces_only = DataLoader(faces, batch_size, True, pin_memory=True)\n","\n","    return dataloader, dataloader_faces_only\n","\n","batch_size = 64\n","dataloader, dataloader_faces_only = load_dataset(batch_size, augment=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-11T00:04:37.714006Z","iopub.status.busy":"2023-06-11T00:04:37.713396Z","iopub.status.idle":"2023-06-11T00:04:37.721915Z","shell.execute_reply":"2023-06-11T00:04:37.721025Z","shell.execute_reply.started":"2023-06-11T00:04:37.713966Z"},"trusted":true},"outputs":[],"source":["len(dataloader_faces_only)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["faces = torch.load('/kaggle/input/ap-cfggan-ds/faces_sex_images_2.pt').permute((0, 3, 1, 2))\n","labels = torch.load('/kaggle/input/ap-cfggan-ds/labels_sex_images_2.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["len(labels[labels == 1]), len(labels[labels == 0])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["men = faces[:3367]\n","indices = torch.randperm(len(men))[:5721-3367]\n","selected_men = men[indices]\n","selected_men = F.hflip(selected_men)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["faces = torch.cat((faces, selected_men), 0)\n","labels = torch.cat((labels, torch.ones(len(selected_men))), 0)\n","faces.shape, labels.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.imshow(F.to_pil_image(F.hflip(selected_men[3])*0.5 + 0.5 ))\n","labels"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oLlJ9HTw53mL","outputId":"01bd67c4-3cf8-4ddb-df56-64893397b8f5"},"outputs":[],"source":["batch_size = 64\n","with open('/kaggle/input/ap-dataset/faces(1).npy','rb') as f:\n","    n_imgs = batch_size * 142\n","    faces = np.load(f)\n","    faces = faces[:n_imgs]#.reshape((n_imgs, 3, 128, 128))\n","    print(faces.shape)\n","    \n","from torch.utils.data import DataLoader\n","dataloader = DataLoader(torch.from_numpy(faces).permute((0, 3, 1, 2)), batch_size, True, pin_memory=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nucb-L3Q53mO","outputId":"d3c8ffde-e8b9-409c-ed11-d76b6aeb5e41","trusted":true},"outputs":[],"source":["next(iter(dataloader))[0].shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T20:28:07.962403Z","iopub.status.busy":"2023-06-10T20:28:07.961914Z","iopub.status.idle":"2023-06-10T20:28:10.873590Z","shell.execute_reply":"2023-06-10T20:28:10.872223Z","shell.execute_reply.started":"2023-06-10T20:28:07.962363Z"},"id":"-QNDJ-Nz53mP","outputId":"4c6b4313-56f3-4af0-d9fa-1049ae3b2048","trusted":true},"outputs":[],"source":["def plot_images(sqr = 5 , class_=1):\n","    plt.figure(figsize = (15,15))\n","    plt.title(\"Real Images\",fontsize = 35)\n","    for i in range(sqr * sqr):\n","        idx = 0 if i >= 64 else i\n","        f = next(iter(dataloader))\n","\n","        while f[1][idx] != class_:\n","            f = next(iter(dataloader))\n","\n","        plt.subplot(sqr,sqr,i+1)\n","        plt.imshow(F.to_pil_image((f[0][idx] + (torch.randn(f[0][idx].size()) * 0.15))*0.5 + 0.5 ))\n","        print(torch.max(f[0][idx] + (torch.randn(f[0][idx].size())*(0.1**0.5))))\n","        plt.xticks([])\n","        plt.yticks([])\n","\n","# to plot images\n","plot_images(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-10T19:12:29.962152Z","iopub.status.busy":"2023-06-10T19:12:29.961800Z","iopub.status.idle":"2023-06-10T19:12:32.536571Z","shell.execute_reply":"2023-06-10T19:12:32.535394Z","shell.execute_reply.started":"2023-06-10T19:12:29.962124Z"},"id":"EdEGfQGJgdWf","outputId":"af26656f-8058-41bd-b300-4bd7750e3158","trusted":true},"outputs":[],"source":["plot_images(5, 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1F7IWyzA53mT","outputId":"06c08ef0-9df8-4a00-c782-32ea0981a9cf"},"outputs":[],"source":["next(iter(dataloader_faces_only)).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rmgyTMIxqqPF","outputId":"33e6f5b5-6fe0-4f65-8973-759b41b4089f"},"outputs":[],"source":["def plot_images(sqr = 5):\n","    plt.figure(figsize = (10,10))\n","    plt.title(\"Real Images\",fontsize = 35)\n","    for i in range(sqr * sqr):\n","        r = next(iter(dataloader_faces_only))\n","        plt.subplot(sqr,sqr,i+1)\n","        plt.imshow(F.to_pil_image(r[i]*0.5+0.5))\n","        plt.xticks([])\n","        plt.yticks([])\n","\n","plot_images(5)"]},{"cell_type":"markdown","metadata":{"id":"drrrJAT0qqPG"},"source":["## Discriminator"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T00:24:26.045516Z","iopub.status.busy":"2023-06-12T00:24:26.045057Z","iopub.status.idle":"2023-06-12T00:24:26.058986Z","shell.execute_reply":"2023-06-12T00:24:26.057993Z","shell.execute_reply.started":"2023-06-12T00:24:26.045477Z"},"id":"R6lkNTY7qqPH","trusted":true},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self, **kwargs):\n","        super().__init__()\n","\n","        self.conv_block1 = self.__block(3, 64)\n","        self.conv_block2 = self.__block(64, 128)\n","        self.conv_block3 = self.__block(128, 256)\n","        self.conv_block4 = self.__block(256, 512)\n","        self.conv_block5 = self.__block(512, 64)\n","        self.linear1 = nn.Sequential(\n","            nn.Linear(1024, 100),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(100, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def __block(self, input, output):\n","        return nn.Sequential(\n","            nn.Conv2d(input, output, 4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(output),\n","            nn.LeakyReLU(0.2)\n","        )\n","\n","    def forward(self, features):\n","        out_conv = self.conv_block1(features)\n","        out_conv = self.conv_block2(out_conv)\n","        out_conv = self.conv_block3(out_conv)\n","        out_conv = self.conv_block4(out_conv)\n","        out_conv = self.conv_block5(out_conv)\n","        \n","        flattened = out_conv.reshape(out_conv.size(0), -1)\n","        \n","        output = self.linear1(flattened)\n","\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"EO1hANd8qqPJ"},"source":["## Generator"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T00:24:32.946608Z","iopub.status.busy":"2023-06-12T00:24:32.946271Z","iopub.status.idle":"2023-06-12T00:24:32.957844Z","shell.execute_reply":"2023-06-12T00:24:32.956874Z","shell.execute_reply.started":"2023-06-12T00:24:32.946581Z"},"trusted":true},"outputs":[],"source":["class Generator(nn.Module):\n","    def __init__(self, latent_dim, **kwargs):\n","        super().__init__()\n","        self.latent_dim = latent_dim\n","        # Upsampling\n","        self.upsampling_block1 = nn.Sequential(\n","            nn.ConvTranspose2d(latent_dim, 512, 4, stride=1, padding=0, bias=False),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.2),\n","            #nn.Dropout(0.2)\n","        )\n","        self.upsampling_block2 = self.__upsampling_block(512, 256)\n","        self.upsampling_block3 = self.__upsampling_block(256, 128)\n","        self.upsampling_block4 = self.__upsampling_block(128, 64)\n","        self.upsampling_block5 = self.__upsampling_block(64, 32)\n","\n","        self.out_layer = nn.Sequential(\n","            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),\n","            nn.Tanh()\n","        )\n","\n","    def __upsampling_block(self, input, output):\n","        return nn.Sequential(\n","            nn.ConvTranspose2d(input, output, 4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(output),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.2)\n","        )\n","\n","    def forward(self, features):\n","        out_upsampling = self.upsampling_block1(features)\n","        out_upsampling = self.upsampling_block2(out_upsampling)\n","        out_upsampling = self.upsampling_block3(out_upsampling)\n","        out_upsampling = self.upsampling_block4(out_upsampling)\n","        out_upsampling = self.upsampling_block5(out_upsampling)\n","        output = self.out_layer(out_upsampling)\n","\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ikzA2KLqqPJ","trusted":true},"outputs":[],"source":["class Generator(nn.Module):\n","    def __init__(self, latent_dim, **kwargs):\n","        super().__init__()\n","        self.latent_dim = latent_dim\n","        self.linear1 = nn.Linear(in_features=latent_dim, out_features=128*128*3, bias=False)\n","    \n","        # Downsampling\n","        self.downsampling_block1 = self.__downsampling_block(3, 128) #64\n","        self.downsampling_block2 = self.__downsampling_block(128, 256)\n","        self.downsampling_block3 = nn.Sequential(\n","            nn.ConvTranspose2d(256, 512, 4, stride=1, padding=2, bias=False),\n","            nn.Conv2d(512, 512, 4, stride=2, padding=2, bias=False),\n","            # nn.BatchNorm2d(output),\n","            nn.LeakyReLU(0.2)\n","        )\n","\n","        # 512x16x16\n","\n","        # Upsampling\n","        self.upsampling_block1 = nn.Sequential(self.__upsampling_block(512, 512), nn.LeakyReLU(0.2))\n","        self.upsampling_block2 = self.__upsampling_block(512, 256)\n","        self.upsampling_block3 = nn.Sequential(\n","            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=2, bias=False),\n","            nn.ConvTranspose2d(128, 128, 4, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(128),\n","            #nn.LeakyReLU(0.2)\n","        )\n","\n","        self.out_layer = nn.Sequential(\n","            nn.ConvTranspose2d(128, 3, 4, stride=1, padding=1),\n","            nn.Tanh()\n","        )\n","\n","    def __downsampling_block(self, input, output):\n","        return nn.Sequential(\n","            nn.Conv2d(input, output, 4, stride=1, padding=1, bias=False),\n","            nn.Conv2d(output, output, 4, stride=2, padding=2, bias=False),\n","            nn.BatchNorm2d(output),\n","            nn.LeakyReLU(0.2)\n","        )\n","\n","    def __upsampling_block(self, input, output):\n","        return nn.Sequential(\n","            nn.ConvTranspose2d(input, output, 4, stride=1, padding=1, bias=False),\n","            nn.ConvTranspose2d(output, output, 4, stride=2, padding=2, bias=False),\n","            nn.BatchNorm2d(output),\n","            #nn.LeakyReLU(0.2)\n","        )\n","\n","    def forward(self, features):\n","        out_linear = self.linear1(features) \n","\n","        reshaped = out_linear.view(-1, 3, 128, 128)\n","        \n","        # Downsampling group\n","        out_downsampling = self.downsampling_block1(reshaped)\n","        out_downsampling = self.downsampling_block2(out_downsampling)\n","        out_downsampling = self.downsampling_block3(out_downsampling)\n","\n","        # Upsampling group\n","        out_upsampling = self.upsampling_block1(out_downsampling) \n","        out_upsampling = self.upsampling_block2(out_upsampling)\n","        out_upsampling = self.upsampling_block3(out_upsampling)\n","        output = self.out_layer(out_upsampling)\n","\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"stkyKznLqqPJ"},"source":["## FGGAN"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T00:24:39.106595Z","iopub.status.busy":"2023-06-12T00:24:39.106251Z","iopub.status.idle":"2023-06-12T00:24:39.131360Z","shell.execute_reply":"2023-06-12T00:24:39.130357Z","shell.execute_reply.started":"2023-06-12T00:24:39.106566Z"},"id":"cjfaC8bQqqPK","trusted":true},"outputs":[],"source":["class FGGAN(nn.Module):\n","    def __init__(self, generator, discriminator, **kwargs):\n","        super().__init__()\n","        self.generator = generator\n","        self.discriminator = discriminator\n","\n","        self.generator.apply(self.weights_init_normal)\n","        self.discriminator.apply(self.weights_init_normal)\n","        \n","        self.g_losses = []\n","        self.d_losses = []\n","\n","    def forward(self, features):\n","        return self.generator(features)\n","    \n","    def compile(self,\n","            generator_optimizer,\n","            discriminator_optimizer,\n","            generator_loss_criterion,\n","            discriminator_loss_criterion\n","        ):\n","        self.generator_optimizer = generator_optimizer\n","        self.discriminator_optimizer = discriminator_optimizer\n","        self.generator_loss_criterion = generator_loss_criterion\n","        self.discriminator_loss_criterion = discriminator_loss_criterion\n","\n","    def train_generator(self, noise, batch_size):\n","        self.generator_optimizer.zero_grad()\n","        generated_output = self.generator(noise)\n","        fake_output = self.discriminator(generated_output)\n","\n","        # Calculate loss\n","        generator_labels = torch.ones(batch_size).float().to(device)\n","        generator_loss = self.generator_loss_criterion(fake_output.squeeze(), generator_labels)\n","\n","        # Update gradients\n","        generator_loss.backward()\n","\n","        # Gradient clipping (exploding gradient)\n","        torch.nn.utils.clip_grad_norm_(self.generator.parameters(), 1)\n","\n","        self.generator_optimizer.step()\n","\n","        g_loss = generator_loss.item()\n","\n","        return g_loss\n","    \n","    def train_discriminator(self, X, noise, batch_size):\n","        self.discriminator_optimizer.zero_grad()\n","        generated_output = self.generator(noise).detach()\n","        fake_output = self.discriminator(generated_output)\n","        real_output = self.discriminator(X)\n","\n","        # Calc losses\n","        discriminator_fake_loss = self.discriminator_loss_criterion(fake_output.squeeze(), torch.zeros(batch_size).float().to(device))\n","        discriminator_real_loss = self.discriminator_loss_criterion(real_output.squeeze(), torch.from_numpy(np.array([0.9]*batch_size)).float().to(device))\n","        \n","        discriminator_loss = discriminator_real_loss + discriminator_fake_loss\n","\n","        # Update gradients\n","        discriminator_loss.backward()\n","\n","        # Gradient clipping\n","        torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), 1)\n","\n","        self.discriminator_optimizer.step()\n","\n","        d_loss = discriminator_loss.item()\n","\n","        return d_loss\n","\n","    def fit(self, X, epochs=10, batch_size=64, latent_dim=100, n_disc=1, n_gen=1):\n","        n_batches = len(X)\n","        batch_print_step = int(n_batches / 10)\n","        print(\"Training starting....\")\n","        #disp_noise = torch.from_numpy(np.random.normal(0, 1, (3, latent_dim))).float().to(device)\n","        disp_noise = torch.randn(4, latent_dim, 1, 1, device=device).float()\n","\n","        for epoch in range(epochs):\n","            g_loss = 0\n","            d_loss = 0\n","\n","            print(f\"Epoch {epoch}/{epochs}: \", end=\"\")\n","            for index, batch in enumerate(X):\n","                batch = batch.to(device)\n","                #noise = torch.from_numpy(np.random.normal(0, 1, (batch_size, latent_dim))).float().to(device)\n","                noise = torch.randn(batch_size, latent_dim, 1, 1, device=device).float()\n","                \n","                for i in range(n_disc):\n","                    d_loss_tmp = self.train_discriminator(batch, noise, batch_size)\n","                d_loss += d_loss_tmp\n","                \n","                for i in range(n_gen):\n","                    g_loss_tmp = self.train_generator(noise, batch_size)\n","                g_loss += g_loss_tmp\n","                if index % batch_print_step == 0:\n","                    print(\"#\", end=\"\")\n","\n","            g_loss /= n_batches\n","            d_loss /= n_batches\n","            \n","            self.g_losses.append(g_loss)\n","            self.d_losses.append(d_loss)\n","\n","\n","            print(f\"\\nGenerator loss: {g_loss}  Discriminator loss: {d_loss}\")\n","            \n","            \n","            with torch.no_grad():\n","                fig, axs = plt.subplots(1, 3)\n","                fig.set_figwidth(12)\n","                fig.set_figheight(4)\n","\n","                out = self.generator(disp_noise.float())\n","                print(out.shape)\n","                #for ax in axs:\n","                axs[0].imshow(np.array(F.to_pil_image(out[0] * 0.5 + 0.5)))\n","                axs[1].imshow(np.array(F.to_pil_image(out[1] * 0.5 + 0.5)))\n","                axs[2].imshow(np.array(F.to_pil_image(out[2] * 0.5 + 0.5)))\n","                plt.show()\n","\n","                img = F.to_pil_image(out[0] * 0.5 + 0.5)\n","                #img.save(f'/content/drive/MyDrive/img_outs/fggan_{epoch}.jpg')\n","            \n","            torch.save(self.state_dict(), './fggan_tmp.pt')\n","            if (epoch+1) % 5 == 0:\n","                torch.save(self.state_dict(), f'./fggan_epoch_{epoch+1}.pt')\n","                with open(f'./fggan_losses_{epoch+1}.npy', 'wb') as f:\n","                    np.save(f, np.array([self.g_losses, self.d_losses]))\n","            \n","    def weights_init_normal(self, m):\n","        classname = m.__class__.__name__\n","        # Apply initial weights to convolutional and linear layers\n","        if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n","            nn.init.normal_(m.weight.data, 0.0,0.02)\n","        return m\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uUMcOILYqqPM"},"source":["## CGAN 1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T00:24:46.239753Z","iopub.status.busy":"2023-06-12T00:24:46.239216Z","iopub.status.idle":"2023-06-12T00:24:46.312926Z","shell.execute_reply":"2023-06-12T00:24:46.311944Z","shell.execute_reply.started":"2023-06-12T00:24:46.239718Z"},"id":"HYheWXIWqqPM","trusted":true},"outputs":[],"source":["from torch.optim.lr_scheduler import LinearLR\n","\n","class cFGGAN(nn.Module):\n","    def __init__(self, generator, discriminator, n_classes=2, **kwargs):\n","        super().__init__()\n","        self.generator = generator\n","        self.discriminator = discriminator\n","        self.n_classes = n_classes\n","\n","        self.generator_conditional_head = self.__conditional_head().to(device)\n","        \"\"\"self.conv_gen = nn.Sequential(\n","            nn.Conv2d(4, 3, 3, stride=1, padding=1, bias=False),\n","            nn.Dropout(0.3),\n","            nn.LeakyReLU(0.2)\n","        )\"\"\"\n","        self.conv_gen = nn.Sequential(\n","            nn.Conv2d(4, 128, 4, stride=1, padding=1, bias=False),\n","            nn.Conv2d(128, 128, 4, stride=2, padding=2, bias=False),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2)\n","        )\n","        self.discriminator_conditional_head = self.__conditional_head().to(device)\n","        \"\"\"self.conv_disc = nn.Sequential(\n","            nn.Conv2d(4, 3, 3, stride=1, padding=1, bias=False),\n","            nn.Dropout(0.3),\n","            nn.LeakyReLU(0.2)\n","        )\"\"\"\n","        self.conv_disc = nn.Sequential(\n","            nn.Conv2d(4, 128, 4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU()\n","        )\n","\n","        self.generator_conditional_head.apply(self.weights_init_normal)\n","        self.conv_gen.apply(self.weights_init_normal)\n","        self.discriminator_conditional_head.apply(self.weights_init_normal)\n","        self.conv_disc.apply(self.weights_init_normal)\n","        # self.discriminator.apply(self.weights_init_normal)\n","        \n","        self.d_losses = []\n","        self.g_losses = []\n","        self.schedule = False\n","\n","    def __conditional_head(self):\n","        return nn.Sequential(\n","            nn.Embedding(self.n_classes, 25),\n","            nn.Linear(25, 128*128*1)\n","        )\n","\n","    def get_generator_parameters(self):\n","        return list(self.generator.parameters()) + list(self.conv_gen.parameters()) + list(self.generator_conditional_head.parameters())\n","        #return list(self.generator.parameters()) + list(self.generator_conditional_head.parameters())\n","\n","    def get_discriminator_parameters(self):\n","        return list(self.discriminator.parameters()) + list(self.conv_disc.parameters()) + list(self.discriminator_conditional_head.parameters())\n","        #return list(self.discriminator.parameters()) + list(self.discriminator_conditional_head.parameters())\n","\n","    def forward(self, features, class_):\n","        # Conditional input\n","        out = self.generator_conditional_head(class_)\n","        reshaped_conditional = out.view(-1, 1, 128, 128).to(device)\n","\n","        # GAN input\n","        out_linear = self.generator.linear1(features) \n","        reshaped_gan = out_linear.view(-1, 3, 128, 128).to(device)\n","        \n","        #conditioned_out = reshaped_gan.to(device) * reshaped_conditional.to(device)\n","        \n","        # Downsampling group\n","        conditioned_out = self.conv_gen(torch.cat((reshaped_gan.to(device), reshaped_conditional.to(device)), 1).to(device))\n","        #out_downsampling = self.generator.downsampling_block1(conditioned_out)\n","        out_downsampling = self.generator.downsampling_block2(conditioned_out)\n","        out_downsampling = self.generator.downsampling_block3(out_downsampling)\n","\n","        # Upsampling group\n","        out_upsampling = self.generator.upsampling_block1(out_downsampling) \n","        out_upsampling = self.generator.upsampling_block2(out_upsampling)\n","        out_upsampling = self.generator.upsampling_block3(out_upsampling)\n","        output = self.generator.out_layer(out_upsampling)\n","\n","        return output\n","\n","    def forward_discriminator(self, features, class_):\n","        # Conditional input\n","        out = self.discriminator_conditional_head(class_)\n","        reshaped_conditional = out.view(-1, 1, 128, 128).to(device)\n","\n","        conditioned_out = self.conv_disc(torch.cat((features.to(device), reshaped_conditional), 1))\n","        #conditioned_out = reshaped_conditional * features\n","        \n","        #out_conv = self.conv_block1(features)\n","        out_conv = self.discriminator.conv_block2(conditioned_out)\n","        out_conv = self.discriminator.conv_block3(out_conv)\n","        out_conv = self.discriminator.conv_block4(out_conv)\n","        out_conv = self.discriminator.conv_block5(out_conv)\n","        \n","        flattened = out_conv.reshape(out_conv.size(0), -1)\n","        \n","        output = self.discriminator.linear1(flattened)\n","        \n","        return output # self.discriminator(conditioned_out)\n","    \n","    def compile(self,\n","            generator_optimizer,\n","            discriminator_optimizer,\n","            generator_loss_criterion,\n","            discriminator_loss_criterion,\n","            schedule=False\n","        ):\n","        self.generator_optimizer = generator_optimizer\n","        self.discriminator_optimizer = discriminator_optimizer\n","        self.generator_loss_criterion = generator_loss_criterion\n","        self.discriminator_loss_criterion = discriminator_loss_criterion\n","        self.schedule = schedule\n","        if schedule:\n","            self.generator_optimizer_scheduler = ExponentialLR(self.generator_optimizer, gamma=0.1, last_epoch=-1, verbose=False)\n","            self.discriminator_optimizer_scheduler = ExponentialLR(self.discriminator_optimizer, gamma=0.1, last_epoch=-1, verbose=False)\n","\n","\n","    def train_generator(self, noise, batch_size):\n","        self.generator_optimizer.zero_grad()\n","        \n","        #random_labels = torch.zeros(batch_size, self.n_classes, 1, 1)\n","        random_labels = torch.randint(0, 1, (batch_size,)).to(device)\n","\n","        generated_output = self(noise, random_labels.int().to(device))\n","        fake_output = self.forward_discriminator(generated_output, random_labels.int())\n","\n","        # Calculate loss\n","        #generator_labels = torch.ones(batch_size).float().to(device)\n","        generator_labels = torch.from_numpy(np.array([0.9] * batch_size)).float().to(device)\n","        generator_loss = self.generator_loss_criterion(fake_output.squeeze(), generator_labels)\n","\n","        # Update gradients\n","        generator_loss.backward()\n","\n","        # Gradient clipping (exploding gradient)\n","        torch.nn.utils.clip_grad_norm_(self.get_generator_parameters(), 1)\n","\n","        self.generator_optimizer.step()\n","        if self.schedule:\n","            self.generator_optimizer_scheduler.step()\n","\n","        g_loss = generator_loss.item()\n","\n","        return g_loss\n","    \n","    def train_discriminator(self, X, labels, noise, batch_size):\n","        self.discriminator_optimizer.zero_grad()\n","        \n","        #random_labels = torch.zeros(batch_size, self.n_classes, 1, 1)\n","        random_labels = torch.randint(0, 1, (batch_size,)).to(device)\n","\n","        generated_output = self(noise, random_labels.int()).detach()\n","        fake_output = self.forward_discriminator(generated_output + torch.randn(tensor.size()) * 1.0 + 0, random_labels.int())\n","        \n","        flip_indices = torch.randperm(len(labels))[:10]\n","        labels[flip_indices] = labels[flip_indices] * (-1) + 1\n","        \n","        real_output = self.forward_discriminator(X + torch.randn(tensor.size()) * 1.0 + 0, labels.int())\n","\n","        # Calc losses\n","        fake_labels = torch.from_numpy(np.array([0.1] * batch_size)).float().to(device)\n","        real_labels = torch.from_numpy(np.array([0.9] * batch_size)).float().to(device)\n","        real_labels[flip_indices] = 0.1\n","        \n","        #discriminator_fake_loss = self.discriminator_loss_criterion(fake_output.squeeze(), torch.zeros(batch_size).float().to(device))\n","        #discriminator_real_loss = self.discriminator_loss_criterion(real_output.squeeze(), torch.ones(batch_size).float().to(device))\n","        discriminator_fake_loss = self.discriminator_loss_criterion(fake_output.squeeze(), fake_labels)\n","        discriminator_real_loss = self.discriminator_loss_criterion(real_output.squeeze(), real_labels)\n","        \n","        discriminator_loss = discriminator_real_loss + discriminator_fake_loss\n","\n","        # Update gradients\n","        discriminator_loss.backward()\n","\n","        # Gradient clipping\n","        torch.nn.utils.clip_grad_norm_(self.get_discriminator_parameters(), 1)\n","\n","        self.discriminator_optimizer.step()\n","        if self.schedule:\n","            self.discriminator_optimizer_scheduler.step()\n","\n","        d_loss = discriminator_loss.item()\n","\n","        return d_loss\n","\n","    def fit(self, X, epochs=10, batch_size=64, latent_dim=100, n_disc=1):\n","        n_batches = len(X)\n","        batch_print_step = int(n_batches / 10)\n","        print(\"Training starting....\")\n","        disp_noise = torch.from_numpy(np.random.normal(0, 1, (4, latent_dim))).float().to(device)\n","        disp_labels = torch.from_numpy(np.array([1, 1, 0, 0])).int().to(device)\n","        \n","\n","        for epoch in range(epochs):\n","            g_loss = 0\n","            d_loss = 0\n","\n","            print(f\"Epoch {epoch}/{epochs}: \", end=\"\")\n","            for index, (batch, labels) in enumerate(X):\n","                batch = batch.to(device)\n","                noise = torch.from_numpy(np.random.normal(0, 1, (batch_size, latent_dim))).float().to(device)\n","\n","                self.train_discriminator(batch, labels.to(device), noise, batch_size)\n","                d_loss += self.train_discriminator(batch, labels.to(device), noise, batch_size)\n","                    \n","                g_loss += self.train_generator(noise.to(device), batch_size)\n","\n","                if index % batch_print_step == 0:\n","                    print(\"#\", end=\"\")\n","\n","            g_loss /= n_batches\n","            d_loss /= n_batches\n","            \n","            self.g_losses.append(g_loss)\n","            self.d_losses.append(d_loss)\n","\n","            print(f\"\\nGenerator loss: {g_loss}  Discriminator loss: {d_loss}\")\n","            \n","            \n","            with torch.no_grad():\n","                fig, axs = plt.subplots(1, 4)\n","                fig.set_figwidth(16)\n","                fig.set_figheight(4)\n","\n","                out = self(disp_noise.float(), disp_labels.int())\n","                print(out.shape)\n","                #for ax in axs:\n","                axs[0].imshow(np.array(F.to_pil_image(out[0] * 0.5 + 0.5)))\n","                axs[1].imshow(np.array(F.to_pil_image(out[1] * 0.5 + 0.5)))\n","                axs[2].imshow(np.array(F.to_pil_image(out[2] * 0.5 + 0.5)))\n","                axs[3].imshow(np.array(F.to_pil_image(out[3] * 0.5 + 0.5)))\n","                plt.show()\n","\n","                img = F.to_pil_image(out[0] * 0.5 + 0.5)\n","                #img.save(f'/content/drive/MyDrive/img_outs/fggan_{epoch}.jpg')\n","            \n","            torch.save(self.state_dict(), './cfggan_tmp.pt')\n","            if (epoch+1) % 5 == 0:\n","                torch.save(self.state_dict(), f'./cfggan_epoch_{epoch+1}.pt')\n","                with open(f'./cfggan_losses_{epoch+1}.npy', 'wb') as f:\n","                    np.save(f, np.array([self.g_losses, self.d_losses]))\n","            \n","    def weights_init_normal(self, m):\n","        classname = m.__class__.__name__\n","        # Apply initial weights to convolutional and linear layers\n","        if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n","            nn.init.normal_(m.weight.data, 0.0,0.02)\n","        if isinstance(m, nn.Embedding):\n","            m.weight.data.normal_(mean=0.0, std=0.02)\n","            if m.padding_idx is not None:\n","                m.weight.data[m.padding_idx].zero_()\n","        if classname.find('BatchNorm') != -1:\n","            nn.init.normal_(m.weight.data, 1.0, 0.02)\n","            nn.init.constant_(m.bias.data, 0)\n","\n","        return m\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## CGAN 2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T00:25:06.905499Z","iopub.status.busy":"2023-06-12T00:25:06.905152Z","iopub.status.idle":"2023-06-12T00:25:06.948747Z","shell.execute_reply":"2023-06-12T00:25:06.947690Z","shell.execute_reply.started":"2023-06-12T00:25:06.905471Z"},"trusted":true},"outputs":[],"source":["from torch.optim.lr_scheduler import LinearLR\n","\n","class cFGGAN(nn.Module):\n","    def __init__(self, generator, discriminator, n_classes=2, n_embbed=32, **kwargs):\n","        super().__init__()\n","        self.generator = generator\n","        self.discriminator = discriminator\n","        self.n_classes = n_classes\n","        self.n_embbed = n_embbed\n","\n","        self.generator_conditional_head = nn.Embedding(self.n_classes, self.n_embbed).to(device)\n","        self.linear_gen = nn.Sequential(\n","            nn.ConvTranspose2d(self.generator.latent_dim+n_embbed, 512, 4, stride=1, padding=0, bias=False),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.2)\n","        ).to(device)\n","        self.discriminator_conditional_head = nn.Embedding(self.n_classes, self.n_embbed).to(device)\n","        self.linear_disc = nn.Sequential(\n","            nn.Linear(1024+self.n_embbed, 100),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(100, 1),\n","            nn.Sigmoid()\n","        ).to(device)\n","\n","        self.generator_conditional_head.apply(self.weights_init_normal)\n","        self.linear_gen.apply(self.weights_init_normal)\n","        self.discriminator_conditional_head.apply(self.weights_init_normal)\n","        self.linear_disc.apply(self.weights_init_normal)\n","        #.generator.apply(self.weights_init_normal)\n","        #self.discriminator.apply(self.weights_init_normal)\n","        \n","        self.d_losses = []\n","        self.g_losses = []\n","        self.schedule = False\n","\n","    def __conditional_head(self):\n","        return nn.Sequential(\n","            nn.Embedding(self.n_classes, 25),\n","            nn.Linear(25, 128*128*1)\n","        )\n","\n","    def get_generator_parameters(self):\n","        return list(self.generator_conditional_head.parameters()) + \\\n","               list(self.linear_gen.parameters()) + \\\n","               list(self.generator.upsampling_block2.parameters()) + \\\n","               list(self.generator.upsampling_block3.parameters()) + \\\n","               list(self.generator.upsampling_block4.parameters()) + \\\n","               list(self.generator.upsampling_block5.parameters()) + \\\n","               list(self.generator.out_layer.parameters())\n","        \n","\n","    def get_discriminator_parameters(self):\n","        return list(self.discriminator_conditional_head.parameters()) + \\\n","               list(self.discriminator.conv_block1.parameters()) + \\\n","               list(self.discriminator.conv_block2.parameters()) + \\\n","               list(self.discriminator.conv_block3.parameters()) + \\\n","               list(self.discriminator.conv_block4.parameters()) + \\\n","               list(self.discriminator.conv_block5.parameters()) + \\\n","               list(self.linear_disc.parameters())\n","\n","    def forward(self, features, class_):\n","        # Conditional input\n","        out = self.generator_conditional_head(class_).view(len(class_), self.n_embbed, 1, 1)\n","        # GAN input\n","        out_linear = self.linear_gen(torch.cat((features, out), 1))\n","        # Upsampling group\n","        out_upsampling = self.generator.upsampling_block2(out_linear) \n","        out_upsampling = self.generator.upsampling_block3(out_upsampling)\n","        out_upsampling = self.generator.upsampling_block4(out_upsampling)\n","        out_upsampling = self.generator.upsampling_block5(out_upsampling)\n","        output = self.generator.out_layer(out_upsampling)\n","\n","        return output\n","\n","    def forward_discriminator(self, features, class_):\n","        # Conditional input\n","        out = self.discriminator_conditional_head(class_)\n","        \n","        out_conv = self.discriminator.conv_block1(features)\n","        out_conv = self.discriminator.conv_block2(out_conv)\n","        out_conv = self.discriminator.conv_block3(out_conv)\n","        out_conv = self.discriminator.conv_block4(out_conv)\n","        out_conv = self.discriminator.conv_block5(out_conv)\n","        \n","        flattened = out_conv.reshape(out_conv.size(0), -1)\n","        \n","        output = self.linear_disc(torch.cat((flattened, out), 1))\n","        \n","        return output # self.discriminator(conditioned_out)\n","    \n","    def compile(self,\n","            generator_optimizer,\n","            discriminator_optimizer,\n","            generator_loss_criterion,\n","            discriminator_loss_criterion,\n","            schedule=False\n","        ):\n","        self.generator_optimizer = generator_optimizer\n","        self.discriminator_optimizer = discriminator_optimizer\n","        self.generator_loss_criterion = generator_loss_criterion\n","        self.discriminator_loss_criterion = discriminator_loss_criterion\n","        self.schedule = schedule\n","        if schedule:\n","            self.generator_optimizer_scheduler = ExponentialLR(self.generator_optimizer, gamma=0.1, last_epoch=-1, verbose=False)\n","            self.discriminator_optimizer_scheduler = ExponentialLR(self.discriminator_optimizer, gamma=0.1, last_epoch=-1, verbose=False)\n","\n","\n","    def train_generator(self, noise, batch_size):\n","        self.generator_optimizer.zero_grad()\n","        \n","        #random_labels = torch.zeros(batch_size, self.n_classes, 1, 1)\n","        random_labels = torch.randint(0, 2, (batch_size,)).to(device)\n","        generated_output = self(noise, random_labels.int().to(device))\n","        fake_output = self.forward_discriminator(generated_output, random_labels.int())\n","\n","        # Calculate loss\n","        #generator_labels = torch.ones(batch_size).float().to(device)\n","        generator_labels = torch.from_numpy(np.array([0.9] * batch_size)).float().to(device)\n","        generator_loss = self.generator_loss_criterion(fake_output.squeeze(), generator_labels)\n","\n","        # Update gradients\n","        generator_loss.backward()\n","\n","        # Gradient clipping (exploding gradient)\n","        torch.nn.utils.clip_grad_norm_(self.get_generator_parameters(), 1)\n","\n","        self.generator_optimizer.step()\n","        if self.schedule:\n","            self.generator_optimizer_scheduler.step()\n","\n","        g_loss = generator_loss.item()\n","\n","        return g_loss\n","    \n","    def train_discriminator(self, X, labels, noise, batch_size):\n","        self.discriminator_optimizer.zero_grad()\n","        \n","        #random_labels = torch.zeros(batch_size, self.n_classes, 1, 1)\n","        random_labels = torch.randint(0, 2, (batch_size,)).to(device)\n","\n","        generated_output = self(noise, random_labels.int()).detach()\n","        fake_output = self.forward_discriminator(generated_output, random_labels.int())\n","        \n","        flip_indices = torch.randperm(len(labels))[:20]\n","        labels[flip_indices] = labels[flip_indices] * (-1) + 1\n","        \n","        real_output = self.forward_discriminator(X, labels.int())\n","\n","        # Calc losses\n","        fake_labels = torch.from_numpy(np.array([0.1] * batch_size)).float().to(device)\n","        real_labels = torch.from_numpy(np.array([0.9] * batch_size)).float().to(device)\n","        real_labels[flip_indices] = 0.1\n","        \n","        #discriminator_fake_loss = self.discriminator_loss_criterion(fake_output.squeeze(), torch.zeros(batch_size).float().to(device))\n","        #discriminator_real_loss = self.discriminator_loss_criterion(real_output.squeeze(), torch.ones(batch_size).float().to(device))\n","        discriminator_fake_loss = self.discriminator_loss_criterion(fake_output.squeeze(), fake_labels)\n","        discriminator_real_loss = self.discriminator_loss_criterion(real_output.squeeze(), real_labels)\n","        \n","        discriminator_loss = discriminator_real_loss + discriminator_fake_loss\n","\n","        # Update gradients\n","        discriminator_loss.backward()\n","\n","        # Gradient clipping\n","        torch.nn.utils.clip_grad_norm_(self.get_discriminator_parameters(), 1)\n","\n","        self.discriminator_optimizer.step()\n","        if self.schedule:\n","            self.discriminator_optimizer_scheduler.step()\n","\n","        d_loss = discriminator_loss.item()\n","\n","        return d_loss\n","\n","    def fit(self, X, epochs=10, batch_size=64, latent_dim=100, n_disc=1, n_gen=1):\n","        n_batches = len(X)\n","        batch_print_step = int(n_batches / 10)\n","        print(\"Training starting....\", batch_size)\n","        #disp_noise = torch.from_numpy(np.random.normal(0, 1, (4, latent_dim))).float().to(device)\n","        disp_noise = torch.randn(4, latent_dim, 1, 1, device=device).float()\n","        disp_labels = torch.from_numpy(np.array([1, 1, 0, 0])).int().to(device)\n","\n","        for epoch in range(epochs):\n","            g_loss = 0\n","            d_loss = 0\n","\n","            print(f\"Epoch {epoch}/{epochs}: \", end=\"\")\n","            for index, (batch, labels) in enumerate(X):\n","                batch = batch.to(device)\n","                #noise = torch.from_numpy(np.random.normal(0, 1, (batch_size, latent_dim))).float().to(device)\n","                noise = torch.randn(batch_size, latent_dim, 1, 1, device=device).float()\n","\n","                #self.train_discriminator(batch, labels.to(device), noise, batch_size)\n","                for i in range(n_disc):\n","                    d_loss_tmp = self.train_discriminator(batch, labels.to(device), noise, batch_size)\n","                d_loss += d_loss_tmp\n","                \n","                for i in range(n_gen):\n","                    g_loss_tmp = self.train_generator(noise.to(device), batch_size)\n","                g_loss += g_loss_tmp\n","                    \n","                if index % batch_print_step == 0:\n","                    print(\"#\", end=\"\")\n","\n","            g_loss /= n_batches\n","            d_loss /= n_batches\n","            \n","            self.g_losses.append(g_loss)\n","            self.d_losses.append(d_loss)\n","\n","            print(f\"\\nGenerator loss: {g_loss}  Discriminator loss: {d_loss}\")\n","            \n","            \n","            with torch.no_grad():\n","                fig, axs = plt.subplots(1, 4)\n","                fig.set_figwidth(16)\n","                fig.set_figheight(4)\n","                out = self(disp_noise.float(), disp_labels.int())\n","                print(out.shape)\n","                #for ax in axs:\n","                axs[0].imshow(np.array(F.to_pil_image(out[0] * 0.5 + 0.5)))\n","                axs[1].imshow(np.array(F.to_pil_image(out[1] * 0.5 + 0.5)))\n","                axs[2].imshow(np.array(F.to_pil_image(out[2] * 0.5 + 0.5)))\n","                axs[3].imshow(np.array(F.to_pil_image(out[3] * 0.5 + 0.5)))\n","                plt.show()\n","\n","                img = F.to_pil_image(out[0] * 0.5 + 0.5)\n","                #img.save(f'/content/drive/MyDrive/img_outs/fggan_{epoch}.jpg')\n","            \n","            torch.save(self.state_dict(), './cfggan_tmp.pt')\n","            if (epoch+1) % 5 == 0:\n","                torch.save(self.state_dict(), f'./cfggan_epoch_{epoch+1}.pt')\n","                with open(f'./cfggan_losses_{epoch+1}.npy', 'wb') as f:\n","                    np.save(f, np.array([self.g_losses, self.d_losses]))\n","            \n","    def weights_init_normal(self, m):\n","        classname = m.__class__.__name__\n","        # Apply initial weights to convolutional and linear layers\n","        if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n","            nn.init.normal_(m.weight.data, 0.0,0.02)\n","        if isinstance(m, nn.Embedding):\n","            m.weight.data.normal_(mean=0.0, std=0.02)\n","            if m.padding_idx is not None:\n","                m.weight.data[m.padding_idx].zero_()\n","        return m\n"]},{"cell_type":"markdown","metadata":{"id":"hTDuNJRQqqPN"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-11T03:02:54.718834Z","iopub.status.busy":"2023-06-11T03:02:54.718159Z","iopub.status.idle":"2023-06-11T03:07:59.960834Z","shell.execute_reply":"2023-06-11T03:07:59.959703Z","shell.execute_reply.started":"2023-06-11T03:02:54.718788Z"},"id":"qPmsYre6P7qp","outputId":"b7d3a341-627e-4a89-ecee-bfde83d2a9fe","trusted":true},"outputs":[],"source":["noise = np.random.normal(0,1,(1, 100))\n","noise = torch.from_numpy(noise).to(device)\n","latent_dim = 100\n","noise = torch.randn(1, latent_dim, 1, 1, device=device).float()\n","\n","generator = Generator(100).to(device)\n","discriminator = Discriminator().to(device)\n","fggan = FGGAN(generator, discriminator)\n","\n","# Load the saved model weights\n","#fggan.load_state_dict(torch.load('/kaggle/input/ap-cfggan-ds/fggan_tmp(4).pt'))\n","\n","generator_optimizer = torch.optim.Adam(\n","    generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n","discriminator_optimizer = torch.optim.Adam(\n","    discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n","\n","generator_criterion = nn.BCELoss().to(device)\n","discriminator_criterion = nn.BCELoss().to(device)\n","\n","fggan.compile(generator_optimizer, discriminator_optimizer,\n","              generator_criterion, discriminator_criterion)\n","\n","out = fggan(noise.float().to(device))\n","print(out.shape)\n","\n","plt.imshow(out[0].permute(1, 2, 0).cpu().detach().numpy()*0.5+0.5)\n","plt.show()\n","\n","fggan.fit(dataloader_faces_only, epochs=10, batch_size=batch_size, n_gen=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T00:52:49.267654Z","iopub.status.busy":"2023-06-12T00:52:49.267301Z","iopub.status.idle":"2023-06-12T00:52:51.718726Z","shell.execute_reply":"2023-06-12T00:52:51.717846Z","shell.execute_reply.started":"2023-06-12T00:52:49.267626Z"},"id":"Nk414-gBqqPO","trusted":true},"outputs":[],"source":["def plot_generated_images(square = 5, epochs = 0): \n","  plt.figure(figsize = (15,15))\n","  for i in range(square * square):\n","    plt.subplot(square, square, i+1)\n","    noise = torch.randn(1, 100, 1, 1, device=device).float()\n","    #img = fggan(noise)[0].permute(1, 2, 0).cpu().detach().numpy()\n","    img = cfggan(noise, torch.from_numpy(np.array([epochs])).to(device))[0].cpu().detach()\n","    plt.imshow(F.to_pil_image(img*0.5 + 0.5 ))\n","    \n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid()\n","plot_generated_images(6, 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T00:52:53.991255Z","iopub.status.busy":"2023-06-12T00:52:53.990877Z","iopub.status.idle":"2023-06-12T00:52:56.823567Z","shell.execute_reply":"2023-06-12T00:52:56.822214Z","shell.execute_reply.started":"2023-06-12T00:52:53.991226Z"},"trusted":true},"outputs":[],"source":["plot_generated_images(6, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-11T03:38:44.851003Z","iopub.status.busy":"2023-06-11T03:38:44.850634Z","iopub.status.idle":"2023-06-11T03:38:47.961217Z","shell.execute_reply":"2023-06-11T03:38:47.960246Z","shell.execute_reply.started":"2023-06-11T03:38:44.850973Z"},"trusted":true},"outputs":[],"source":["def plot_generated_images(square = 5, epochs = 0): \n","    plt.figure(figsize = (10,10))\n","    for i in range(square * square):\n","        plt.subplot(square, square, i+1)\n","        noise = torch.randn(1, 100, 1, 1, device=device).float()\n","        #img = fggan(noise)[0].permute(1, 2, 0).cpu().detach().numpy()\n","        img = fggan(noise).to(device)[0].cpu().detach()\n","        plt.imshow(F.to_pil_image(img*0.5 + 0.5 ))\n","\n","        plt.xticks([])\n","        plt.yticks([])\n","        plt.grid()\n","plot_generated_images(6)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i7aVjGJtqqPP","trusted":true},"outputs":[],"source":["del generator\n","del discriminator\n","del fggan\n","torch.cuda.empty_cache()\n","import gc\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["DATASET STUFF"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
